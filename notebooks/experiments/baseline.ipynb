{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f9f23872-e012-45c5-b600-634984c75518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/src\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/src\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "import hashlib\n",
    "from functools import cached_property, wraps\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from custom.config_types import CONFIG_TYPES\n",
    "from logger import Logger\n",
    "from pytorch_pfn_extras.config import Config\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from util import load_yaml, reduce_mem_usage, sort_df_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c21224e2-21c5-43b3-8cb2-14e80d2f7720",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(name=\"baseline\")\n",
    "\n",
    "# set config\n",
    "pre_eval_config = load_yaml()\n",
    "config = Config(pre_eval_config, types=CONFIG_TYPES)\n",
    "\n",
    "# set const\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dce8c6-a035-4d1e-95ad-fcd276156878",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f0cd3c81-25eb-4a9f-a65e-5c918b9fc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskDatset:\n",
    "    def __init__(self, config, overwrite=False) -> None:\n",
    "        self.config = config\n",
    "        self.dirpath = Path(config[\"/global/resources\"]) / \"input\"\n",
    "        self.dataset_name = config[\"/fe/dataset\"]\n",
    "\n",
    "        self.raw_train_filepath = self.dirpath / f\"{self.dataset_name}_raw_train.parquet\"\n",
    "        self.raw_test_filepath = self.dirpath / f\"{self.dataset_name}_raw_test.parquet\"\n",
    "\n",
    "        self.overwrite = overwrite\n",
    "\n",
    "    @property\n",
    "    def raw_train_data(self):\n",
    "        if self.raw_train_filepath.is_file() and (not self.overwrite):\n",
    "            return pd.read_parquet(self.raw_train_filepath)\n",
    "\n",
    "        uids = self.raw_data.query(\"x != 999\")[\"uid\"].unique()\n",
    "        raw_train_df = self.raw_data[self.raw_data[\"uid\"].isin(uids)].reset_index(drop=True)\n",
    "        raw_train_df.to_parquet(self.raw_train_filepath)\n",
    "        return raw_train_df\n",
    "\n",
    "    @property\n",
    "    def raw_test_data(self):\n",
    "        if self.raw_test_filepath.is_file() and (not self.overwrite):\n",
    "            return pd.read_parquet(self.raw_test_filepath)\n",
    "\n",
    "        uids = self.raw_data.query(\"x == 999\")[\"uid\"].unique()\n",
    "        raw_test_df = self.raw_data[self.raw_data[\"uid\"].isin(uids)].reset_index(drop=True)\n",
    "        raw_test_df.to_parquet(self.raw_test_filepath)\n",
    "        return raw_test_df\n",
    "\n",
    "    @cached_property\n",
    "    def raw_data(self):\n",
    "        return read_parquet_from_csv(\n",
    "            filepath=self.dirpath / f\"{self.dataset_name}.csv.gz\",\n",
    "            dirpath=self.dirpath,\n",
    "            process_fns=[reduce_mem_usage, sort_df_numpy],\n",
    "            overwrite=self.config[\"/fe/overwrite\"],\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def poi_data(self):\n",
    "        return read_parquet_from_csv(\n",
    "            filepath=self.dirpath / \"cell_POIcat.csv.gz\", dirpath=self.dirpath\n",
    "        )\n",
    "\n",
    "\n",
    "def read_parquet_from_csv(\n",
    "    filepath: Path,\n",
    "    dirpath: Path,\n",
    "    process_fns: list[Callable] | None = None,\n",
    "    overwrite: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    name = filepath.name.split(\".\")[0]\n",
    "    parquet_filepath = dirpath / f\"{name}.parquet\"\n",
    "    if parquet_filepath.is_file() and (not overwrite):\n",
    "        logger.info(f\"load parquet file ({str(filepath)})\")\n",
    "        return pd.read_parquet(parquet_filepath)\n",
    "\n",
    "    logger.info(f\"load csv & convert to parquet ({str(filepath)})\")\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    if process_fns is not None:\n",
    "        for fn in process_fns:\n",
    "            logger.info(f\"excute {fn.__name__}\")\n",
    "            df = fn(df)\n",
    "\n",
    "    df.to_parquet(parquet_filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5d5cba9b-857d-4cb7-808b-6975dd271fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-06 21:32:46] \u001b[32m[baseline] [INFO] - load parquet file (/workspace/resources/input/task2_dataset.csv.gz)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "task_dataset = TaskDatset(config, overwrite=True)\n",
    "raw_train_df = task_dataset.raw_train_data\n",
    "\n",
    "if DEBUG:\n",
    "    user_ids = raw_train_df[\"uid\"].sample(100, random_state=config[\"/global/seed\"]).tolist()\n",
    "    raw_train_df = raw_train_df[raw_train_df[\"uid\"].isin(user_ids)].reset_index(drop=True)\n",
    "\n",
    "raw_test_df = task_dataset.raw_test_data\n",
    "train_df = raw_train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "380d575d-f862-4981-b10f-052ff895057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(df: pd.DataFrame, group_key: str, group_values: list[str]):\n",
    "    grouped = df.groupby(group_key, sort=False)\n",
    "    sequences = [torch.tensor(group[group_values].to_numpy()) for _, group in grouped]\n",
    "    return sequences\n",
    "\n",
    "\n",
    "# feature_names = [x for x in train_df.columns if x.startswith(\"f_\")]\n",
    "feature_seqs = make_sequences(df=train_df, group_key=\"uid\", group_values=[\"d\", \"t\"])\n",
    "auxiliary_seqs = make_sequences(\n",
    "    df=train_df.query(\"d >= 60\"), group_key=\"uid\", group_values=[\"d\", \"t\"]\n",
    ")  # features for prediction zone\n",
    "\n",
    "target_seqs = make_sequences(\n",
    "    df=train_df.query(\"d >= 60\"),\n",
    "    group_key=\"uid\",\n",
    "    group_values=[\"x\", \"y\"],\n",
    ")  # target is x & y over 60 zone\n",
    "\n",
    "len(feature_seqs), len(target_seqs), len(auxiliary_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f89f58db-f46c-41f4-949a-98f747bcc408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, feature_seqs, auxiliary_seqs, target_seqs):\n",
    "        self.feature_seqs = feature_seqs\n",
    "        self.auxiliary_seqs = auxiliary_seqs\n",
    "        self.target_seqs = target_seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_seqs)\n",
    "\n",
    "    def __getitem__(self, index: int) -> dict[str : torch.Tensor]:\n",
    "        feature_seqs = torch.Tensor(self.feature_seqs[index]).float()\n",
    "        auxiliary_seqs = torch.Tensor(self.auxiliary_seqs[index]).float()\n",
    "        target_seqs = torch.Tensor(self.target_seqs[index]).float()\n",
    "        return {\n",
    "            \"feature_seqs\": feature_seqs,\n",
    "            \"auxiliary_seqs\": auxiliary_seqs,\n",
    "            \"target_seqs\": target_seqs,\n",
    "        }\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, feature_seqs):\n",
    "        self.feature_seqs = feature_seqs\n",
    "        self.auxiliary_seqs = auxiliary_seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_seqs)\n",
    "\n",
    "    def __getitem__(self, index: int) -> dict[str : torch.Tensor]:\n",
    "        feature_seqs = torch.Tensor(self.feature_seqs[index]).float()\n",
    "        auxiliary_seqs = torch.Tensor(self.auxiliary_seqs[index]).float()\n",
    "        return {\n",
    "            \"feature_seqs\": feature_seqs,\n",
    "            \"auxiliary_seqs\": auxiliary_seqs,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "018b5b12-e488-46e7-a844-22d38f2562c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequenceCollateFn:\n",
    "    def __init__(self, is_train_mode=True):\n",
    "        self.is_train_mode = is_train_mode\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        feature_seqs = [item[\"feature_seqs\"] for item in batch]\n",
    "        auxiliary_seqs = [item[\"auxiliary_seqs\"] for item in batch]\n",
    "        feature_lengths = [len(seq) for seq in feature_seqs]\n",
    "        auxiliary_lengths = [len(seq) for seq in auxiliary_seqs]\n",
    "\n",
    "        feature_seqs_padded = pad_sequence(\n",
    "            [(seq) for seq in feature_seqs], batch_first=True\n",
    "        )  # (sequence_len, feature_dim)\n",
    "        auxiliary_seqs_padded = pad_sequence(\n",
    "            [(seq) for seq in auxiliary_seqs], batch_first=True\n",
    "        )  # (sequence_len, feature_dim)\n",
    "\n",
    "        if not self.is_train_mode:\n",
    "            return {\n",
    "                \"feature_seqs\": feature_seqs_padded,\n",
    "                \"auxiliary_seqs\": auxiliary_seqs_padded,\n",
    "                \"feature_lengths\": feature_lengths,\n",
    "                \"auxiliary_lengths\": auxiliary_lengths,\n",
    "            }\n",
    "\n",
    "        target_seqs = [item[\"target_seqs\"] for item in batch]\n",
    "        target_seqs_padded = pad_sequence(\n",
    "            [(seq) for seq in target_seqs], batch_first=True\n",
    "        )  # (sequence_len, target_dim)\n",
    "        return {\n",
    "            \"feature_seqs\": feature_seqs_padded,\n",
    "            \"auxiliary_seqs\": auxiliary_seqs_padded,\n",
    "            \"target_seqs\": target_seqs_padded,\n",
    "            \"feature_lengths\": feature_lengths,\n",
    "            \"auxiliary_lengths\": auxiliary_lengths,\n",
    "        }\n",
    "\n",
    "\n",
    "def to_device(batch, device):\n",
    "    for k, v in batch.items():\n",
    "        if not k.endswith(\"lengths\"):\n",
    "            batch[k] = v.to(device)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def train_fn(config, wandb_logger=None):\n",
    "    model = config[\"/model\"]\n",
    "    dataloader = config[\"/dataloader/train\"]\n",
    "    criterion = config[\"/criterion\"]\n",
    "    optimizer = config[\"/optimizer\"]\n",
    "    scheduler = config[\"/scheduler\"]\n",
    "\n",
    "    # training settings\n",
    "    device = config[\"/nn/device\"]\n",
    "    use_amp = config[\"/nn/fp16\"]\n",
    "    gradient_accumulation_steps = config[\"/nn/gradient_accumulation_steps\"]\n",
    "    clip_grad_norm = config[\"/nn/clip_grad_norm\"]\n",
    "\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    losses = []\n",
    "\n",
    "    iteration_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, batch in iteration_bar:\n",
    "        batch = to_device(batch, device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            batch_outputs = model(batch)\n",
    "            loss = criterion(batch_outputs, batch)\n",
    "            loss = torch.div(loss, gradient_accumulation_steps)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if config.clip_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if config.batch_scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "        if wandb_logger is not None:\n",
    "            wandb_logger.log({\"train_loss\": loss, \"lr\": scheduler.get_lr()[0]})\n",
    "        losses.append(float(loss))\n",
    "        iteration_bar.set_description(\n",
    "            f\"loss: {np.mean(losses):.4f} lr: {scheduler.get_lr()[0]:.6f}\"\n",
    "        )\n",
    "\n",
    "    loss = np.mean(losses)\n",
    "    return {\"loss\": loss, \"step\": step}\n",
    "\n",
    "\n",
    "def valid_fn(config):\n",
    "    model = config[\"/model\"]\n",
    "    dataloader = config[\"/dataloader/valid\"]\n",
    "    criterion = config[\"/criterion\"]\n",
    "\n",
    "    # training settings\n",
    "    device = config[\"/nn/device\"]\n",
    "    gradient_accumulation_steps = config[\"/nn/gradient_accumulation_steps\"]\n",
    "\n",
    "    model.eval()\n",
    "    outputs, losses = [], []\n",
    "\n",
    "    iteration_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for _, batch in iteration_bar:\n",
    "        batch = to_device(batch, device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = model(batch)\n",
    "            loss = criterion(batch_outputs, batch)\n",
    "            loss = torch.div(loss, gradient_accumulation_steps)\n",
    "\n",
    "        batch_outputs = batch_outputs.to(\"cpu\").numpy()\n",
    "        outputs.append(batch_outputs)\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        iteration_bar.set_description(f\"loss: {np.mean(losses):.4f}\")\n",
    "\n",
    "    outputs = np.concatenate(outputs)\n",
    "    loss = np.mean(losses)\n",
    "    return {\"loss\": loss, \"outputs\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "51bc4ac1-74ee-4053-ba59-7689e85f667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_eval_config = load_yaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4d178d23-417e-4800-95f7-b69ee6966777",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pre_eval_config[\"nn\"][\"device\"] = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "381a1e22-c683-46a7-9048-5a255c768c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size1, input_size2, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size1, hidden_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size2, hidden_size, batch_first=True)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # to variable length\n",
    "        s1 = pack_padded_sequence(\n",
    "            batch[\"feature_seqs\"],\n",
    "            batch[\"feature_lengths\"],\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        s2 = pack_padded_sequence(\n",
    "            batch[\"auxiliary_seqs\"],\n",
    "            batch[\"auxiliary_lengths\"],\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        x1, (hn_1, cn_1) = self.lstm1(s1)\n",
    "\n",
    "        # Use the final hidden and cell state of lstm1 as initial state for lstm2\n",
    "        x2, _ = self.lstm2(s2, (hn_1, cn_1))\n",
    "        x, _ = pad_packed_sequence(x2, batch_first=True)  # to fixible length\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f08a8448-2319-4cf9-a3be-18e99b50195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 2\n",
    "batch_size = 2\n",
    "\n",
    "input_size1 = 2\n",
    "input_size2 = 2\n",
    "output_size = 2\n",
    "hidden_size = 2\n",
    "\n",
    "train_dataset = TrainDataset(\n",
    "    feature_seqs=feature_seqs, auxiliary_seqs=auxiliary_seqs, target_seqs=target_seqs\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=PadSequenceCollateFn(is_train_mode=True),\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "model = CustomLSTMModel(\n",
    "    input_size1=input_size1,\n",
    "    input_size2=input_size2,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "df048c0d-3d8e-4a1c-a93e-20e9f095d58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.88it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "iteration_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "for _, batch in iteration_bar:\n",
    "    batch = to_device(batch, device)\n",
    "    output = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2ff34d3b-4522-4b84-86d3-aa37e9fc8781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 218, 2])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848648b6-1dc2-4942-b8b4-22cadc224ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
