global:
  project: "HuMob-Challenge-2023"
  seed: 8823
  debug: False
  resources: "/workspace/resources"
  padding_value: -999

fe:
  out_dir: "002/features"
  overwrite: False
  dataset: "task2_dataset"
  use_poi_features: True
  poi_decomposer:
    type: NMF
    n_components: 10
    random_state: "@/global/seed"
  cycles: [7, 14]
  extractors:
    - {
        type: GroupedDiffFeatureExtractor,
        group_key: uid,
        group_values: ["d", "x", "y"],
        intervals: [1, -1, 2, -2],
      }
    - {
        type: GroupedDiffFeatureExtractor,
        group_key: ["uid", "d"],
        group_values: ["t"],
        intervals: [1, -1, 2, -2],
      }
    - {
        type: GroupedShiftFeatureExtractor,
        group_key: uid,
        group_values: ["d", "x", "y"],
        intervals: [1, -1, 2, -2],
      }
    - {
        type: GroupedShiftFeatureExtractor,
        group_key: ["uid", "d"],
        group_values: ["t"],
        intervals: [1, -1, 2, -2],
      }
    - {
        type: GroupedSimpleFeatureExtoractor,
        group_key: ["uid"],
        group_values: ["x", "y"],
        agg_methods: ["min", "max", "mean", "median"],
      }
    - {
        type: GroupedSimpleFeatureExtoractor,
        group_key: ["uid", "d"],
        group_values: ["x", "y"],
        agg_methods: ["min", "max", "mean", "median"],
      }
    - { type: RawFeatureExtractor, use_columns: ["d", "t", "x", "y"] } # + poi features

  regression_target_transform: log
  scaling:
    type: StandardScaler

cv:
  num_fold: 5
  valid_folds: [0]
  strategy:
    type: StratifiedGroupKFold
    n_splits: "@/cv/num_fold"
    shuffle: True
    random_state: "@/global/seed"

nn:
  out_dir: "002/models"
  device: ???
  padding_value: "@/global/padding_value"
  max_epochs: 4
  gradient_accumulation_steps: 4
  clip_grad_norm: 1
  fp16: True
  batch_scheduler: True
  iters_per_epoch: ???
  num_training_steps: ???

  feature:
    feature_names: ???
    auxiliary_names: ??? # f_d_*, f_t_*
    target_names: ["x", "y"]

  model:
    type: DynamicGraphLSTMV1
    in_features_sage: ??? # node feature
    hidden_features_sage: 1024
    embed_features_sage: 512
    input_size1_lstm: ???
    input_size2_lstm: ???
    hidden_size_lstm: 1024
    output_size: ???

  criterion:
    type: SeqMSELoss

  metrics:
    type: MSEMetric

  dataset:
    train:
      type: TrainDatasetV02
      feature_seqs: ???
      auxiliary_seqs: ???
      target_seqs: ???
      central_node_feature_seqs: ???
      neighbor_node_feature_seqs: ???
    valid:
      type: TrainDatasetV02
      feature_seqs: ???
      auxiliary_seqs: ???
      target_seqs: ???
      central_node_feature_seqs: ???
      neighbor_node_feature_seqs: ???
    test:
      type: TestDatasetV02
      feature_seqs: ???
      auxiliary_seqs: ???
      central_node_feature_seqs: ???
      neighbor_node_feature_seqs: ???

  dataloader:
    train:
      type: DataLoader
      dataset: "@/nn/dataset/train"
      collate_fn:
        type: PadSequenceWithNodeCollateFn
        is_train_mode: True
        padding_value: "@/nn/padding_value"
        return_padding_mask: False
      batch_size: 8
      num_workers: 1
      shuffle: False
      pin_memory: True
      drop_last: True
    valid:
      type: DataLoader
      dataset: "@/nn/dataset/valid"
      collate_fn:
        type: PadSequenceWithNodeCollateFn
        is_train_mode: True
        padding_value: "@/nn/padding_value"
        return_padding_mask: False
      batch_size: 32
      num_workers: 1
      shuffle: False
      pin_memory: True
      drop_last: False
    test:
      type: DataLoader
      dataset: "@/nn/dataset/test"
      collate_fn:
        type: PadSequenceWithNodeCollateFn
        is_train_mode: False
        padding_value: "@/nn/padding_value"
        return_padding_mask: False
      batch_size: 32
      num_workers: 1
      shuffle: False
      pin_memory: True
      drop_last: False

  optimizer:
    type: AdamW
    params: { type: method_call, obj: "@/nn/model", method: parameters }
    lr: 5.0e-2
    weight_decay: 0
    eps: 1.0e-6
    betas: [0.9, 0.999]

  scheduler:
    type: get_cosine_schedule_with_warmup
    optimizer: "@/nn/optimizer"
    num_warmup_steps: 0
    num_training_steps: "@/nn/num_training_steps"
    num_cycles: 0.5
